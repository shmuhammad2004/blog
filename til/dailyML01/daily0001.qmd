---
title: "Bias and Varience in Machine Learning Algorithms"
date: '2022-05-10'
image: varience.png
output: html_document
---

![](./varience.png)


The accuracy of a any prediction depends on two quantities: reducible error and irreducible error. irreducible error is an error. Irreducible error is an error that is not reducible while reducible error is an error that is reducible. 

In general, predictive error of machine learning models can be divided into three categories: Irreducible error, Bias error, and Variene Error.


### Irreducible error

As the name suggest, irreducible error is an error that is not reducible due to the unknown variables.  Unlike reducible error, which we can reduce by using appropriate learning algorithms. On the other hand, variene error and bias error are errors that are reducible.

### Bias error

Bias are the prior assumptions made by a model to make the target function easier to learn. For example, linear algorithm assumes that the target function is linear. If the target function is non-linear, then the model will have bias error. Though, they are easy to learn and understand, but they are not flexible and have low lower predictive performance compare to complex or flexible algorithms. 

Therefore, less-flexible algorithms (e.g., Linear Regression, Linear Discriminant Analysis and Logistic Regression) have higher bias than complex or flexible algorithms (e.g., Neural Networks, Decision Trees, k-Nearest Neighbors and Support Vector Machines.).

## Variance Error

Assume we train the same model with a two different training dataset: N and M. Variance refers to the changes in the model when different training data set is used. Certainly, the model will have variance error when the training dataset is different. But, the error should not be too high between different training datasets.

Therefore, low-varience algorithms have small changes when different training dataset is used and high-varience algorithms have large changes when different training dataset is used. Flexible or complex algorithms have a high variance (e.g SVM, Decision Trees, Neural Networks, k-Nearest Neighbors). However, non-flexible algorithms have a low variance (e.g., Linear Regression, Linear Discriminant Analysis and Logistic Regression). 


Finally, the goal of any supervised learning algorithm is to achieve low-bias and low-variance error.  Achieveing these goals is the key to the success of any supervised learning algorithm. But, how can we achieve these goals? That is where the idea of Bias-Variance Tradeoff comes into play.

## Summary 
 - Non-complex machine learning algorithms have high bias and low variance.
 - Complex/flexible machine learning algorithms have a low bias but a high variance